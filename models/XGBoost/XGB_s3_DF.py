# -*- coding: utf-8 -*-
"""epic_s3_allfold.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vOBPn69ob1AZj_lQcqiayRmnMY9vBvik
"""

import os
import time
import sys
import pandas as pd
import numpy as np
from pathlib import Path

import glob
import re

# Data split & Tuning params
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import GroupKFold, GroupShuffleSplit
from sklearn.model_selection import PredefinedSplit
from sklearn.model_selection import train_test_split

from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error

# XGBoost
from xgboost import XGBRegressor

# save model
import joblib

import hyperopt
from hyperopt import fmin, tpe, hp, anneal, Trials
from sklearn.model_selection import cross_val_score

############################################################################
# Define a function to get RMSE and save hyperopt results and the model:
def get_rmse_save_result (trials, best, X_train):
  # print condition
  if best is best1:
    condition = "numID"

  print("--------------------------")
  print("Results for "+condition+":")
    

  # Train the model on the full training set with the best hyperparameters
  best_model = XGBRegressor(random_state=random_state,
                            n_estimators=int(best['n_estimators']), eta=best['eta'],
                            max_depth=int(best['max_depth']), gamma=best['gamma'],
                            subsample=best['subsample'],
                            colsample_bytree=best['colsample_bytree'],
                            reg_alpha= best['reg_alpha'], 
                            reg_lambda=best['reg_lambda'],
                            eval_metric="rmse",tree_method='gpu_hist', gpu_id=0)
  best_model.fit(X_train, y_train)

  # save the best model as a file
  joblib.dump(best_model, 'Model_s3_XGB_DF_allfold_alpha.joblib')
  
  # Save Hyperopt Iterations -----------------------------
  # extract
  rmse_df = pd.DataFrame(trials.results)
  rmse_df.columns = ['RMSE', 'status']
  # save
  result = pd.concat([pd.DataFrame(trials.vals), rmse_df], axis=1)
  result.to_csv('Hyper_s3_XGB_DF_allfold_alpha.csv', index=False)
  # Get summary statistics for the 'RMSE' column
  print("--- RMSE on TRAIN set ---")
  RMSE = result['RMSE'].describe()
  print(RMSE)
  # Print Mean and SD of RMSE
  print("Train RMSE Best:", RMSE['min'])
  print("Train RMSE Mean:", RMSE['mean'])
  print("Train RMSE SD:", RMSE['std'])
  
############################################################################

# Read Data ------------------------------------------------------------------

# Set the working directory
path = "./"
os.chdir(path)

file_list = ["train_scenario_3_fold_0_dynfeatures.csv", 
             "train_scenario_3_fold_1_dynfeatures.csv", 
             "train_scenario_3_fold_2_dynfeatures.csv", 
             "train_scenario_3_fold_3_dynfeatures.csv"]

total = pd.DataFrame()

# concact data from different folds
for file_name in file_list:
    fold_data = re.findall('fold_''\d+',file_name)
    data = pd.read_csv(file_name)
    data['fold'] = fold_data[0]
    # Concatenate the data frames vertically
    total = pd.concat([total, data], axis=0)
    
print(total['fold'].value_counts()) # the counts are correct

print("Before drop duplicated:", total.shape)
total = total.drop(['fold'], axis=1).drop_duplicates()
# reset index
total = total.reset_index(drop = True)
print('After drop duplicated',total.shape)

# add a variable that indicates data from same elicitor
def create_elicitor(x):
    if x in [16, 20]:
        return 0
    elif x in [0, 3]:
        return 1
    elif x in [22, 10]:
        return 2
    elif x in [4, 21]:
        return 3
    
total['elicitor'] = total['VIDEO_FILENAME'].apply(create_elicitor)

# Handling Missing data -------------------------------------------------
# total = total.groupby(['unique_number']).ffill().bfill()  # this drop 'unique_number'

# add a variable that indicates data from same (subject X elicitor)
# to split data into train data set and validation data set later
#total['unique_number_s3'] = total['ID'].astype(str) + total['elicitor'].astype(str)

# Handling Missing data -------------------------------------------------
unique_number = total['unique_number']
total = total.groupby('unique_number').ffill().bfill() # this drop 'unique_number'
total['unique_number'] = unique_number                 # add back

# Drop features------------------------------------------
total.drop(['SCENARIO'], axis=1, inplace=True)


# Check Data ---------------------------------------------------------
print("ID:", total['ID'].unique())
print("Video ID:", total['VIDEO_FILENAME'].unique())
print("Elicitor:", total['elicitor'].unique())
#print("ID&Elicitor:", total['unique_number_s3'].unique())
print("--------------------------------")

#print(total['unique_number_s3'].nunique())
print("ID:", total['ID'].nunique())
print("Elicitor:", total['elicitor'].nunique())
print("Video ID:", total['VIDEO_FILENAME'].nunique())

# # Split TRAIN vs. TEST ------------------------------------------------------
train = total

# Split Target vs. Predictors -------------------------------------------

# select target 
target_col = ['valence', 'arousal']
# split train
y_train = train[target_col]
X_train = train.drop(target_col, axis=1)
# # split test
# y_test = test[target_col]
# X_test = test.drop(target_col, axis=1)

print("X_train.columns:", X_train.columns)

# Specifiy GroupKfold CV ----------------------------------------------------------
gkf = GroupKFold(n_splits=4)
print("--------------------------------")
print("Check 3 sub folds in train data:")
for i, (train_index2, valid_index2) in enumerate(gkf.split(X_train, y_train, groups=train['elicitor'])):
 print(f"Fold {i}:")
 print(f"  Train: index={train_index2}, elicitor={train['elicitor'][train_index2].unique()}")
 print(f"  Valid: index={valid_index2}, elicitor={train['elicitor'][valid_index2].unique()}")



# TUNE --------------------------------------------------------------------

###### Hyperopt #######--------------------------------------------------------
max_eval = 100 # number of iterations in hyperopt: use a smaller num for testing code.

# Step 1: Initialize space of values:

# # Space for exploring hyperparameters:
space={'max_depth': hp.quniform("max_depth", 3, 12, 1),
        'gamma': hp.uniform ('gamma', 0, 9),
        'subsample': hp.uniform('subsample', 0.5, 1),
        'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),
        'n_estimators': hp.quniform('n_estimators', 50, 300, 1),
        'eta': hp.quniform('eta', 0.05, 0.5, 0.025),
        'reg_alpha' :  hp.quniform('reg_alpha', 0, 50, 0.1),
       'reg_lambda' : hp.quniform('reg_lambda', 0, 50, 0.1)
       }

# Toy space for testing code:
#space={'max_depth': hp.quniform("max_depth", 3, 9, 1),
#      'gamma': hp.uniform ('gamma', 0, 0),
#      'subsample': hp.uniform('subsample', 0.99, 1),
#      'colsample_bytree': hp.uniform('colsample_bytree', 0.99, 1),
#      'n_estimators': hp.quniform('n_estimators', 3, 5, 1),
#      'eta': hp.quniform('eta', 0.2, 0.3, 0.025)
#     }

# --------------------------------------------------------------------------

# Set the working directory
save_path = "./" 
os.chdir(save_path)

print("--------------------------")
print("Numeric ID")

# Step 2: Define objective function:
def hyperparam_tuning1(space):
  model = XGBRegressor(n_estimators=int(space['n_estimators']),
                     eta=space['eta'],
                     max_depth=int(space['max_depth']),
                     gamma=space['gamma'],
                     subsample=space['subsample'],
                     colsample_bytree=space['colsample_bytree'],
                     reg_alpha = space['reg_alpha'],
                     reg_lambda= space['reg_lambda'],
                     eval_metric="rmse", seed=12435,tree_method='gpu_hist', gpu_id=0)
  # cross validation
  score = -cross_val_score(estimator=model, 
                                 X = X_train, y = y_train, 
                                 groups = train['elicitor'],
                                 cv=gkf,
                                 scoring='neg_root_mean_squared_error',
                                 n_jobs = -1).mean()
  return score

# Step 3: Run Hyperopt function:
random_state=42
start = time.time()
trials1 = Trials()
best1 = fmin(fn=hyperparam_tuning1,
            space=space,
            algo=tpe.suggest,
            max_evals=max_eval,
            trials=trials1,
            rstate=np.random.default_rng(random_state))
print('It takes %s minutes' % ((time.time() - start)/60)) 
print ("Best params:", best1)


get_rmse_save_result(trials1, best1, X_train)
